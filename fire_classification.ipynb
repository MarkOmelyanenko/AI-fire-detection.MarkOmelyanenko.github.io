{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y2OYJUZg_C2"
      },
      "source": [
        "# **Hardware: check that the GPU is selected**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-gRbBoGg_C4"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXkRPpkng_C4"
      },
      "source": [
        "# **Connecting to the Wandb platform** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHtoSoYwg_C4"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XHLaajSg_C5"
      },
      "source": [
        "# **Importing libraries** ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgFv8sz-RepI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "!pip install pytorch_lightning\n",
        "from torchmetrics.classification import MulticlassConfusionMatrix, Accuracy\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger, TensorBoardLogger , CSVLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAG94q93WW44"
      },
      "source": [
        "# **Installation of the ‘benchamark’ library to evaluate results in detail** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3f7Iqjtg-v-"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_bench\n",
        "from pytorch_bench import benchmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmOVZGI7g_C5"
      },
      "source": [
        "# **Downloading the training data set** ##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTg3XPzsg_C5"
      },
      "outputs": [],
      "source": [
        "!rm -rf FIRE\n",
        "!rm -rf sample_data\n",
        "!mkdir -p FIRE/train\n",
        "\n",
        "!wget https://nextcloud.ig.umons.ac.be/s/KaqzczZsXfsnMER/download/FIRE_DATABASE_3.zip\n",
        "!unzip FIRE_DATABASE_3.zip -d FIRE/train/DB3\n",
        "!rm FIRE_DATABASE_3.zip\n",
        "\n",
        "# DB1 : https://nextcloud.ig.umons.ac.be/s/REWbK6K4XRtoeNw/download/FIRE_DATABASE_1.zip\n",
        "# DB2 : https://nextcloud.ig.umons.ac.be/s/faKyDy7LCxfz9Xk/download/FIRE_DATABASE_2.zip\n",
        "# DB3 : https://nextcloud.ig.umons.ac.be/s/KaqzczZsXfsnMER/download/FIRE_DATABASE_3.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB2gFnK7g_C5"
      },
      "source": [
        "# **Downloading the test data set** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9rYb3uig_C5"
      },
      "outputs": [],
      "source": [
        "!wget https://nextcloud.ig.umons.ac.be/s/XqEMQtqQNPoG2cY/download/test.zip\n",
        "!unzip test.zip -d FIRE/test\n",
        "!rm test.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJl0axMvRzBA"
      },
      "source": [
        "#**Setting up Hyper-parameters(training parameters)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "MRARa6e7RBfv"
      },
      "outputs": [],
      "source": [
        "Train_data_path = \"DB3/FIRE_DATABASE_3\" # @param [\"small\",\"DB1/FIRE_DATABASE_1\",\"DB2/FIRE_DATABASE_2\",\"DB3/FIRE_DATABASE_3\"]\n",
        "Train_data_path = os.path.join('FIRE/train', Train_data_path)\n",
        "Test_data_path = \"test\" #@param [\"test\",\"test_defi1\"]\n",
        "Test_data_path = os.path.join('FIRE/test', Test_data_path)\n",
        "Batch_size=128 #@param [8,16,32,64,128,256] {type:\"raw\"}\n",
        "Epochs=50 #@param [2,5,10,20,50,100,200] {type:\"raw\"}\n",
        "Learning_rate = 0.001 #@param [0.1, 0.01,0.02,0.05,0.001,0.002,0.005] {type:\"raw\"}\n",
        "Train_split = 0.7 #@param [0.7,0.8,0.9] {type:\"raw\"}\n",
        "Img_size = 224 #@param [224,299] {type:\"raw\"}\n",
        "Accelerator= \"auto\" #@param [\"cpu\",\"gpu\",\"auto\"]\n",
        "num_classes = 3\n",
        "LOG_DIR=\"logs/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htCxd_-rMOF1"
      },
      "source": [
        "#**Download VGG16 pre-trained model & view architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAbUmzfDMRwZ"
      },
      "outputs": [],
      "source": [
        "vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "print(vgg16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-r-8hwHg_C5"
      },
      "source": [
        "# **Define model, Forward/Backward & Transfer Leraning parameters** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "4lYcbdh8g_C5"
      },
      "outputs": [],
      "source": [
        "class FireDetectionModel(pl.LightningModule):\n",
        "    def __init__(self, num_classes=num_classes, learning_rate=Learning_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.confusion_matrix = MulticlassConfusionMatrix(num_classes=num_classes)\n",
        "\n",
        "        self.model_vgg16 = vgg16\n",
        "\n",
        "        for param in self.model_vgg16.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.model_vgg16.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_classes = num_classes\n",
        "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model_vgg16(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        loss = self.criterion(outputs, labels)\n",
        "        acc = self.train_accuracy(outputs, labels)\n",
        "\n",
        "        self.log_dict({'train_loss':loss,\"train_acc\":acc}, on_step=True,prog_bar=True,logger=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.train_accuracy.reset()\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs, labels = batch\n",
        "        outputs = self(inputs)\n",
        "        val_loss = self.criterion(outputs, labels)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        val_acc = self.val_accuracy(predicted, labels)\n",
        "        self.log_dict({'val_loss':val_loss,\"val_acc\":val_acc},prog_bar=True, on_step=False, on_epoch=True)\n",
        "        return {'val_loss': val_loss, 'val_acc': val_acc}\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.val_accuracy.reset()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        outputs = self(x)\n",
        "        test_loss = F.cross_entropy(outputs, y)\n",
        "        test_acc = self.test_accuracy(outputs, y)\n",
        "        self.log_dict({'test_loss': test_loss, \"test_acc\": test_acc}, prog_bar = True, on_step = False, on_epoch = True)\n",
        "        self.confusion_matrix.update(outputs.argmax(dim = 1), y)\n",
        "        return {'test_loss': test_loss, 'test_acc': test_acc}\n",
        "\n",
        "    def on_test_end(self):\n",
        "        self.test_accuracy.reset()\n",
        "        fig_, ax_ = self.confusion_matrix.plot()\n",
        "        self.confusion_matrix.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        #return torch.optim.SGD(self.model.classifier.parameters(), lr=self.learning_rate)\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUIN3rERg_C5"
      },
      "source": [
        "# **Display some images using the ‘display_class_images’ function** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "JVACaYbUg_C5"
      },
      "outputs": [],
      "source": [
        "def display_class_images(class_path):\n",
        "  import glob\n",
        "  import matplotlib.image as mpimg\n",
        "  images = []\n",
        "  for img_path in glob.glob(class_path):\n",
        "      images.append(mpimg.imread(img_path))\n",
        "  plt.figure(figsize=(14,12))\n",
        "  columns = 4\n",
        "  for i, img in enumerate(images):\n",
        "      if (i<=4):\n",
        "        img=cv2.resize(img, (256,256))\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaUh8C49Vbut"
      },
      "outputs": [],
      "source": [
        "display_class_images(Train_data_path + \"/start_fire/*.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO272F_eg_C6"
      },
      "source": [
        "# **Creation of training data sets, validation and ‘Data Loaders’ testing** #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "bm3D84Qag_C6"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(dataset_path, testset_path, batch_size, train_split, img_size):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(img_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    print(f\"Train data path: {Train_data_path}\")\n",
        "    print(f\"Test data path: {Test_data_path}\")\n",
        "\n",
        "    dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(train_split * dataset_size))\n",
        "    np.random.shuffle(indices)\n",
        "    train_indices, val_indices = indices[:split], indices[split:]\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    test_dataset = datasets.ImageFolder(testset_path, transform=test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    print(f\"Train size: {len(train_indices)}, Val size: {len(val_indices)}\")\n",
        "    print(f\"Dataset path: {dataset_path}, Testset path: {testset_path}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nL67lzZCU3"
      },
      "source": [
        "# **Define Hyper-parameters, EarlyStopping, Checkpoints and WandDB parameters** #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEPBYtpzg_C6"
      },
      "outputs": [],
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project=\"fire-detection\", config={\n",
        "    \"learning_rate\": Learning_rate,\n",
        "    \"epochs\": Epochs,\n",
        "    \"batch_size\": Batch_size,\n",
        "    \"model\": \"vgg16\"\n",
        "})\n",
        "\n",
        "# Create data loaders\n",
        "train_loader, val_loader, test_loader = create_data_loaders(\n",
        "    Train_data_path, Test_data_path, Batch_size, Train_split, Img_size\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model_vgg16 = FireDetectionModel(num_classes=3, learning_rate=Learning_rate)\n",
        "\n",
        "# Setup callbacks\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath='checkpoints',\n",
        "    filename='best-checkpoint',\n",
        "    save_top_k=1,\n",
        "    mode='min'\n",
        ")\n",
        "early_stop_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize WandbLogger\n",
        "wandb_logger = WandbLogger(project=\"fire-detection\")\n",
        "\n",
        "# Initialize WandbLogger\n",
        "csv_logger = CSVLogger(LOG_DIR, name=\"cnn\", version='')\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=Epochs,\n",
        "    accelerator=Accelerator,\n",
        "    log_every_n_steps=1,\n",
        "    devices=1,\n",
        "    logger=[wandb_logger, csv_logger],\n",
        "    callbacks=[checkpoint_callback, early_stop_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fONYY_mgZnEC"
      },
      "source": [
        "# **Start training** #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9oaIX-jjZfjX"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.fit(model_vgg16, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfXJJCDYZuJw"
      },
      "source": [
        "# **Evaluating the model** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0nSfgf5Zsle"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "trainer.test(model_vgg16, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqx_D49Rg_C6"
      },
      "source": [
        "# **Display training curves using the ‘plot_metrics’ function** ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "RWymHIeMg_C6"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(log_folder):\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  # Load the CSV file generated by CSVLogger\n",
        "  df = pd.read_csv(f'{LOG_DIR}/{log_folder}/metrics.csv')\n",
        "  train_df = df[df['train_loss_epoch'].notna()]\n",
        "  val_df = df[df['val_loss'].notna()]\n",
        "\n",
        "  # Plot training loss\n",
        "  plt.plot(train_df['epoch'], train_df['train_loss_epoch'], label='Train Loss')\n",
        "  plt.plot(val_df['epoch'], val_df['val_loss'], label='Validation Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training & Validation Loss')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training accuracy\n",
        "  plt.plot(train_df['epoch'], train_df['train_acc_epoch'], label='Train Acc')\n",
        "  plt.plot(val_df['epoch'], val_df['val_acc'], label='Val Acc')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training & Validation Accuracy')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxbyZZmjYE6G"
      },
      "outputs": [],
      "source": [
        "plot_metrics('cnn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TZOXbaGcte0"
      },
      "source": [
        "# **Evaluation of the model using various metrics from ‘Benchmark’ library** #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jafeQWmhYH_v"
      },
      "outputs": [],
      "source": [
        "example_input = torch.randn(1, 3, Img_size, Img_size)\n",
        "results = benchmark(model_vgg16, example_input)\n",
        "\n",
        "# log to wandb\n",
        "wandb.log({\n",
        "    \"benchmark\": results\n",
        "})\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrCejFSOc4pl"
      },
      "source": [
        "# **Test the classification model with a test image of your choice\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbaigZQDjR2P"
      },
      "outputs": [],
      "source": [
        "image_path = \"YOUR IMAGE\"\n",
        "\n",
        "classes = [\"fire\", \"no fire\", \"start fire\"]\n",
        "\n",
        "# Load and preprocess the image\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((Img_size, Img_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "x = transform(img).unsqueeze(0).to(\"cuda\")  # Add batch dimension\n",
        "\n",
        "model_vgg16.to(\"cuda\")\n",
        "\n",
        "# Predict\n",
        "model_vgg16.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model_vgg16(x)\n",
        "    probabilities = torch.nn.functional.softmax(pred[0], dim=0)\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(10, 10))\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "for pos, prob in enumerate(probabilities):\n",
        "    class_name = classes[pos]\n",
        "    print(f\"Class Name: {class_name} --- Class Probability: {prob.item()*100:.2f}%\")\n",
        "\n",
        "    if pos == torch.argmax(probabilities).item():\n",
        "        font = cv2.FONT_HERSHEY_COMPLEX\n",
        "        textsize = cv2.getTextSize(class_name, font, 2, 3)[0]\n",
        "        textX = (img.shape[1] - textsize[0]) // 2\n",
        "        textY = (img.shape[0] + textsize[1]) // 2\n",
        "        cv2.putText(img, f\"{class_name}: {prob.item()*100:.2f}%\", (textX-50, textY), font, 2, (255,0,0), 6, cv2.LINE_AA)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5HpkX1QWqYy"
      },
      "source": [
        "# **Test the classification model with a test video of your choice\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldpbp3gHZT3E"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "video_path = \"YOUR FILE.mp4\"  # Path to the incoming video\n",
        "output_path = \"output_video.mp4\"  # Path to the output video\n",
        "classes = [\"fire\", \"no fire\", \"start fire\"]\n",
        "\n",
        "# Loading the model\n",
        "model_vgg16.to(\"cuda\")  # Moving the model to the GPU\n",
        "model_vgg16.eval()  # Setting to evaluation mode\n",
        "\n",
        "# Transformations for every frame\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((Img_size, Img_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Open video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Video could not be opened\")\n",
        "    exit()\n",
        "\n",
        "# Get information about the video\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Preparing to record the processed video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Video has ended\n",
        "\n",
        "    # Frame conversion\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    x = transform(img).unsqueeze(0).to(\"cuda\")  # Adding a batch size\n",
        "\n",
        "    # Prediction\n",
        "    with torch.no_grad():\n",
        "        pred = model_vgg16(x)\n",
        "        probabilities = torch.nn.functional.softmax(pred[0], dim=0)\n",
        "\n",
        "    # Determining the class with the highest probability\n",
        "    predicted_class = torch.argmax(probabilities).item()\n",
        "    class_name = classes[predicted_class]\n",
        "    confidence = probabilities[predicted_class].item() * 100\n",
        "\n",
        "    # Add text to a frame\n",
        "    font = cv2.FONT_HERSHEY_COMPLEX\n",
        "    text = f\"{class_name}: {confidence:.2f}%\"\n",
        "    cv2.putText(frame, text, (50, 50), font, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Recording the processed frame\n",
        "    out.write(frame)\n",
        "\n",
        "# Freeing up resources\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(\"Processing completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmG_cEBuXsRQ"
      },
      "source": [
        "# **XAI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DT-LK7TwXrEN"
      },
      "outputs": [],
      "source": [
        "!pip install captum\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "7sbLg2BbXuEh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import torch.nn as nn\n",
        "from captum.attr import LayerGradCam\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "RK7Loc-oX5OB"
      },
      "outputs": [],
      "source": [
        "img_size = 224\n",
        "testset_path  = \"FIRE/test/test/\"\n",
        "batch_size = 1\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "test_dataset = datasets.ImageFolder(testset_path, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKJA8Sl1Zg-R"
      },
      "outputs": [],
      "source": [
        "def get_last_conv_layer(model):\n",
        "    last_conv_layer = None\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            last_conv_layer = module\n",
        "    if last_conv_layer is None:\n",
        "        raise ValueError(\"No Conv2d layer found in the model.\")\n",
        "    return last_conv_layer\n",
        "\n",
        "def visualize_gradcam(model, inputs, labels, device, figsize=(18, 6)):\n",
        "    # Get last convolutional layer\n",
        "    last_conv_layer = get_last_conv_layer(model)\n",
        "    gradcam = LayerGradCam(model, last_conv_layer)\n",
        "\n",
        "    # Convert labels if necessary\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = int(torch.argmax(labels.detach()).cpu().numpy())\n",
        "\n",
        "    model_vgg16.to(\"cuda\")\n",
        "\n",
        "    # Compute Grad-CAM attribution\n",
        "    attribution = gradcam.attribute(inputs, target=labels, relu_attributions=True)\n",
        "\n",
        "    # Convert tensors to numpy arrays\n",
        "    attribution_np = attribution[0].cpu().detach().numpy()\n",
        "    inputs_np = inputs[0].cpu().permute(1, 2, 0).detach().numpy()\n",
        "\n",
        "    # Get original image dimensions\n",
        "    height, width = inputs_np.shape[:2]\n",
        "\n",
        "    # Resize attribution map to match input image size\n",
        "    heatmap = cv2.resize(attribution_np[0], (width, height))\n",
        "\n",
        "    # Normalize heatmap\n",
        "    heatmap = np.maximum(heatmap, 0)\n",
        "    heatmap = heatmap / np.max(heatmap)\n",
        "\n",
        "    # Plot results\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(inputs_np)\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Heatmap only\n",
        "    im = axes[1].imshow(heatmap, cmap='jet')\n",
        "    axes[1].set_title('Attention Heatmap')\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im, ax=axes[1])\n",
        "\n",
        "    # Overlay\n",
        "    axes[2].imshow(inputs_np)\n",
        "    axes[2].imshow(heatmap, alpha=0.5, cmap='jet')\n",
        "    axes[2].set_title('Overlay')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "#inputs, labels = next(iter(test_loader))\n",
        "#inputs = inputs.to(device)\n",
        "\n",
        "index = 271  # Index of the image\n",
        "inputs, labels = test_dataset[index]\n",
        "inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "fig = visualize_gradcam(model_vgg16, inputs, labels, device)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxBzQgV4rTyK"
      },
      "source": [
        "# **Localisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3zQ3__yrTfl"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "k8RwZ9JWrbtU"
      },
      "outputs": [],
      "source": [
        "nb_classes       = 2\n",
        "batch_size       = 8 #@param [8,16,32,64] {type:\"raw\"}\n",
        "epochs           = 5 #@param [5, 10,20,50,100,200] {type:\"raw\"}\n",
        "dataset_path     = \"/content/fire_detection_dataset/D-Fire\"\n",
        "input_dim        = 640 #@param [640] {type:\"raw\"}\n",
        "train_dataset    = \"images/train\"\n",
        "test_dataset     = \"images/test\"\n",
        "valid_dataset    = \"images/valid\"\n",
        "yaml_config_name = 'dataset.yaml'\n",
        "project_path     = \"/content/drive/MyDrive/fire/fire_detection\"\n",
        "yolo_version     = \"yolov8s\" #@param [\"yolov8n\", \"yolov8s\",\"yolov8m\",\"yolov8l\",\"yolov8x\"] {type:\"string\"}\n",
        "classes          =  ['smoke', 'fire']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMc8UMXht5Uc"
      },
      "outputs": [],
      "source": [
        "!wget https://nextcloud.ig.umons.ac.be/s/8QNxNrPEEQyE9tN/download/d_fire.zip\n",
        "!unzip d_fire.zip -d /content/fire_detection_dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "onOFMRqGt7e6"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "def generate_yolo_config(output_path):\n",
        "    # Define the configuration\n",
        "    config = {\n",
        "        \"path\": dataset_path,  # dataset root dir\n",
        "        \"train\": train_dataset,  # train images (relative to 'path')\n",
        "        \"val\": valid_dataset,  # val images (relative to 'path')\n",
        "        \"test\": test_dataset,\n",
        "        'names': classes\n",
        "    }\n",
        "\n",
        "    # Write the configuration to a .yaml file\n",
        "    with open(output_path, 'w') as file:\n",
        "        yaml.dump(config, file, default_flow_style=False)\n",
        "\n",
        "generate_yolo_config(yaml_config_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_PzkIrot9U7"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(f'{yolo_version}.pt')\n",
        "\n",
        "# Train model\n",
        "results = model.train(data='dataset.yaml', epochs=epochs, imgsz=input_dim)\n",
        "\n",
        "# Evaluate model\n",
        "results = model.val()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3TINywYuAfz"
      },
      "outputs": [],
      "source": [
        "results = model(f'{dataset_path}/images/train/AoF00015.jpg')\n",
        "\n",
        "for result in results:\n",
        "    boxes = result.boxes\n",
        "    masks = result.masks\n",
        "    result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the localisation model with a test video of your choice\"**"
      ],
      "metadata": {
        "id": "QPXK78ZZz72L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvaEw-jK8o1K"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "video_path = \"YOUR FILE\"  # Path to the incoming video\n",
        "output_path = \"output_yolo.mp4\"  # Path to the output video\n",
        "classes = [\"smoke\", \"fire\"]  # Classes\n",
        "\n",
        "# Завантаження моделі\n",
        "model.to(\"cuda\")  # Moving the model to the GPU\n",
        "\n",
        "# Open a video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Video could not be opened\")\n",
        "    exit()\n",
        "\n",
        "# Get information about a video\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Preparing to record the processed video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Video has ended\n",
        "\n",
        "    # YoloV8 prediction\n",
        "    results = model(frame, device=\"cuda\")  # Transferring the frame to the inferno\n",
        "\n",
        "    # Getting predictions\n",
        "    for result in results:\n",
        "        boxes = result.boxes  # Bounding rectangles\n",
        "        for box in boxes:\n",
        "            # Getting coordinates, class, and probability\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Rectangle coordinates\n",
        "            cls = int(box.cls[0])  # Class\n",
        "            confidence = float(box.conf[0]) * 100  # Probability\n",
        "\n",
        "            # Add text and rectangle to a frame\n",
        "            label = f\"{classes[cls]}: {confidence:.2f}%\"\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Rectangle\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Recording the processed frame\n",
        "    out.write(frame)\n",
        "\n",
        "# Freeing up resources\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(\"Processing completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the classification + localisation model with a test video of your choice\"**"
      ],
      "metadata": {
        "id": "HGL4tPpg0OON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgNtgzxUAgul"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "video_path = \"start_fire.mp4\"  # Path to the incoming video\n",
        "output_path = \"out_fire_smoke(1).mp4\"  # Path to the output video\n",
        "\n",
        "classes_vgg16 = [\"fire\", \"no fire\", \"start fire\"]  # VGG16 classes\n",
        "classes_yolo = [\"smoke\", \"fire\"]  # YoloV8 classes\n",
        "\n",
        "# Loading model\n",
        "model_vgg16.to(\"cuda\")  # Moving the model to the GPU\n",
        "model.to(\"cuda\")  # Moving the model to the GPU\n",
        "\n",
        "model_vgg16.eval()\n",
        "\n",
        "# Transformations for every frame\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((Img_size, Img_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Open a video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "    print(\"Video could not be opened\")\n",
        "    exit()\n",
        "\n",
        "# Get information about a video\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Preparing to record the processed video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Video has ended\n",
        "\n",
        "    # Frame conversion\n",
        "    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    x = transform(img).unsqueeze(0).to(\"cuda\")  # Adding a batch size\n",
        "\n",
        "    # Prediction\n",
        "    with torch.no_grad():\n",
        "        pred = model_vgg16(x)\n",
        "        probabilities = torch.nn.functional.softmax(pred[0], dim=0)\n",
        "\n",
        "    # Determining the class with the highest probability\n",
        "    predicted_class = torch.argmax(probabilities).item()\n",
        "    class_name = classes_vgg16[predicted_class]\n",
        "    confidence = probabilities[predicted_class].item() * 100\n",
        "\n",
        "    # Add text to a frame\n",
        "    font = cv2.FONT_HERSHEY_COMPLEX\n",
        "    text = f\"{class_name}: {confidence:.2f}%\"\n",
        "    cv2.putText(frame, text, (50, 50), font, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # YoloV8 predictions\n",
        "    results = model(frame, device=\"cuda\")  # Transferring the frame to the inferno\n",
        "\n",
        "    # Getting predictions\n",
        "    for result in results:\n",
        "        boxes = result.boxes  # Bounding rectangles\n",
        "        for box in boxes:\n",
        "            # Getting coordinates, class, and probability\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Rectangle coordinates\n",
        "            cls = int(box.cls[0])  # Клас\n",
        "            confidence = float(box.conf[0]) * 100  # Probability\n",
        "\n",
        "            # Add a rectangle and text\n",
        "            label = f\"{classes_yolo[cls]}: {confidence:.2f}%\"\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Rectangle\n",
        "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Recording the processed frame\n",
        "    out.write(frame)\n",
        "\n",
        "# Freeing up resources\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(\"Processing completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}